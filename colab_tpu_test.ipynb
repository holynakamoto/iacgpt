{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# IaC-GPT TPU Training (Colab/Kaggle Prototype)\n\nTest nanochat training on TPUs before production deployment.\n\n**Setup:**\n1. Runtime → Change runtime type → TPU\n2. Run all cells\n\n**TPU Support:**\n- Colab: TPU v2-8 (8 cores, 64GB HBM) or v3-8 (8 cores, 128GB HBM)\n- Kaggle: TPU v5e-1 (1 core, 16GB HBM) or v5e-8 (8 cores, 128GB HBM)\n- Native bfloat16 support\n- ~5-10x faster than T4 GPUs for transformer training"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Install dependencies using uv (better dependency resolution than pip)\n# Step 1: Install uv\n!curl -LsSf https://astral.sh/uv/install.sh | sh\n!source $HOME/.cargo/env\n\n# Step 2: Use uv to install all dependencies (handles conflicts automatically)\n!~/.cargo/bin/uv pip install --system \\\n    torch==2.9.0 \\\n    torch-xla==2.9.0 \\\n    cloud-tpu-client \\\n    \"google-api-core>=2.27.0\" \\\n    \"google-cloud-storage>=3.9.0\" \\\n    \"protobuf>=4.25.2,<6.0\" \\\n    tiktoken pyarrow filelock rustbpe wandb tabulate regex zstandard pyyaml\n\nprint(\"✅ Installation complete via uv\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone nanochat repo\n",
    "!git clone https://github.com/holynakamoto/iacgpt.git nanochat 2>/dev/null || \\\n",
    "    (cd nanochat && git pull origin master)\n",
    "%cd nanochat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Verify TPU detection (updated for torch-xla 2.9.0 API)\nimport torch\nimport torch_xla\nimport torch_xla.core.xla_model as xm\nimport torch_xla.runtime as xr\n\nprint(\"=\" * 70)\nprint(\"TPU DETECTION TEST\")\nprint(\"=\" * 70)\n\n# Use new torch_xla 2.9.0 API\ndevice = torch_xla.device()\nprint(f\"TPU device: {device}\")\nprint(f\"Number of TPU cores: {xr.world_size()}\")\nprint(f\"Local ordinal: {xr.local_ordinal()}\")\nprint(f\"Global ordinal: {xr.global_ordinal()}\")\n\n# Test tensor operation\nx = torch.randn(3, 3, device=device)\ny = x @ x.t()\nprint(f\"\\nTest matmul: {y.shape}\")\nprint(f\"Device type: {y.device}\")\nprint(\"=\" * 70)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add TPU Support to common.py\n",
    "\n",
    "We need to patch the device detection to recognize TPUs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Patch common.py to add TPU support (torch-xla 2.9.0 API)\nimport os\n\ntpu_patch = '''\ndef autodetect_device_type():\n    # Check for TPU first (Colab, Kaggle)\n    try:\n        import torch_xla\n        import torch_xla.runtime as xr\n        device = torch_xla.device()\n        device_type = \"xla\"\n        print0(f\"Autodetected device type: {device_type} (TPU with {xr.world_size()} cores)\")\n        return device_type\n    except ImportError:\n        pass\n    except Exception as e:\n        print0(f\"TPU detection failed: {e}\")\n    \n    # Fallback to CUDA/MPS/CPU\n    if torch.cuda.is_available():\n        device_type = \"cuda\"\n    elif torch.backends.mps.is_available():\n        device_type = \"mps\"\n    else:\n        device_type = \"cpu\"\n    print0(f\"Autodetected device type: {device_type}\")\n    return device_type\n'''\n\n# Read current common.py\nwith open('common.py', 'r') as f:\n    content = f.read()\n\n# Replace autodetect_device_type function\nimport re\npattern = r'def autodetect_device_type\\(\\):.*?return device_type'\ncontent = re.sub(pattern, tpu_patch.strip(), content, flags=re.DOTALL)\n\nwith open('common.py', 'w') as f:\n    f.write(content)\n\nprint(\"✅ Patched common.py with TPU support (torch-xla 2.9.0 API)\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare IaC Training Data\n",
    "\n",
    "Same data pipeline as GPU training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, glob, subprocess\n",
    "\n",
    "CACHE_DIR = os.path.expanduser(\"~/.cache/nanochat\")\n",
    "DATA_DIR = os.path.join(CACHE_DIR, \"iac_data\")\n",
    "BASE_DATA = os.path.join(CACHE_DIR, \"base_data\")\n",
    "\n",
    "# Quick data prep (minimal dataset for testing)\n",
    "print(\"Preparing minimal IaC dataset for TPU testing...\")\n",
    "subprocess.run([\"bash\", \"dev/fast_scrape_iac.sh\"], input=b\"n\", check=True)\n",
    "\n",
    "# Convert to training shards\n",
    "subprocess.run([\n",
    "    \"python3\", \"dev/repackage_iac_data.py\",\n",
    "    \"--input-dir\", \"data/iac_raw_cloned\",\n",
    "    \"--output-dir\", DATA_DIR,\n",
    "    \"--include-synthetic\", \"--include-docs\"\n",
    "], check=True)\n",
    "\n",
    "# Link base_data\n",
    "if os.path.islink(BASE_DATA):\n",
    "    os.unlink(BASE_DATA)\n",
    "os.symlink(DATA_DIR, BASE_DATA)\n",
    "\n",
    "print(f\"✅ Data ready: {len(glob.glob(f'{BASE_DATA}/*.parquet'))} shards\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train BPE tokenizer\n",
    "!python3 -m scripts.tok_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train on TPU (XLA)\n",
    "\n",
    "Use torch_xla's distributed launcher instead of torchrun."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# TPU training command (updated for single-core v5e-1)\n# Note: This requires modifications to base_train.py to use XLA\n\nMODEL_DEPTH = 12\nBATCH_SIZE = 8  # v5e-1 has 16GB HBM, can handle larger batches\nNUM_CORES = 1   # v5e-1 is single-core\n\n# For single-core TPU, use regular python (not xla_dist)\ncmd = f\"\"\"python3 scripts/base_train.py \\\n    --depth={MODEL_DEPTH} \\\n    --device-batch-size={BATCH_SIZE} \\\n    --window-pattern=L \\\n    --target-param-data-ratio=8 \\\n    --run=dummy \\\n    --model-tag=iac-gpt-tpu-d{MODEL_DEPTH} \\\n    --eval-every=100 \\\n    --sample-every=100 \\\n    --save-every=100\"\"\"\n\nprint(\"=\" * 80)\nprint(\"TPU v5e-1 TRAINING COMMAND (Single Core):\")\nprint(cmd)\nprint(\"=\" * 80)\nprint(\"\\n⚠️  Note: base_train.py needs XLA modifications first!\")\nprint(\"Next step: Patch base_train.py for XLA compatibility\\n\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "To complete TPU support:\n",
    "\n",
    "1. **Modify base_train.py:**\n",
    "   - Replace `torch.distributed` with `torch_xla.distributed`\n",
    "   - Use `xm.optimizer_step(optimizer)` instead of `optimizer.step()`\n",
    "   - Use `xm.all_reduce()` for gradient synchronization\n",
    "\n",
    "2. **Modify engine.py:**\n",
    "   - Add XLA-specific compilation flags\n",
    "   - Use `xm.mark_step()` after backward pass\n",
    "\n",
    "3. **Test on Colab TPU v2-8**\n",
    "\n",
    "4. **Port to Kaggle for TPU v5e-8**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}